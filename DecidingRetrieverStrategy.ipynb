{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastchat.serve.inference import load_model\n",
    "from fastchat.conversation import conv_templates\n",
    "import re\n",
    "import torch\n",
    "\n",
    "class model:\n",
    "    def __init__(self, path='C:\\\\Users\\\\vloba\\\\OneDrive\\\\Projects\\\\TFM\\\\vicuna13b', device='cuda'):\n",
    "        self.model, self.tokenizer = load_model(\n",
    "            model_path = path,\n",
    "            device = device,\n",
    "            num_gpus = 1,\n",
    "            max_gpu_memory = '24Gib',\n",
    "            load_8bit = True,\n",
    "        )\n",
    "        self.device = device\n",
    "        \n",
    "    def prompt(self, question, context):\n",
    "        answer = context + '\\n\\n' + \\\n",
    "            'Sé breve, claro y conciso.\\n' + \\\n",
    "            question \n",
    "        return answer\n",
    "    \n",
    "    def context (self, docs):\n",
    "        context=' '.join([doc for doc in docs])\n",
    "        return re.sub('\\n', ' ', context)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def query(self, question):\n",
    "        conv = conv_templates[\"vicuna_v1.1\"].copy()\n",
    "        conv.append_message(conv.roles[0], question)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        input_ids = self.tokenizer([prompt]).input_ids\n",
    "        output_ids = self.model.generate(\n",
    "                torch.as_tensor(input_ids).to(self.device),\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=200,\n",
    "            )\n",
    "\n",
    "        output_ids = output_ids[0][len(input_ids[0]):]\n",
    "\n",
    "        outputs = self.tokenizer.decode(\n",
    "            output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "        )\n",
    "\n",
    "        conv.correct_message(question)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import CrossEncoder\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class tester:\n",
    "    def __init__(self, model, path, chunk_size=[600], k=[5]):\n",
    "        self.model = model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.k = k\n",
    "        self.loader = PyPDFDirectoryLoader(path)\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name='hiiamsid/sentence_similarity_spanish_es')\n",
    "        self.cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')\n",
    "    \n",
    "    def test(self, df, n=1):\n",
    "        assert len(self.chunk_size) == len(self.k), 'chunk_size and k must have the same length'\n",
    "        for i in range(len(self.chunk_size)):\n",
    "            chunk_size = self.chunk_size[i]\n",
    "            k = self.k[i]\n",
    "            print(f'chunk_size: {chunk_size}, k: {k}')\n",
    "            print('Splitting the documents')\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_size//5)\n",
    "            documents = splitter.split_documents(self.loader.load())\n",
    "            print('Calculating the embeddings')\n",
    "            docsearch = Chroma.from_documents(documents, self.embeddings)\n",
    "            for j in range(n):\n",
    "                print(f'Iteration {j+1}')\n",
    "                start = time.time()\n",
    "                answers = []\n",
    "                print('Starting the test')\n",
    "                for index, row in df.iterrows():\n",
    "                    #calculating the answers\n",
    "                    question = row['question'] + '\\na. ' + row['option A'] + '\\nb. ' + row['option B'] + '\\nc. ' + row['option C'] + '\\nd. ' + row['option D']\n",
    "                    docs = [doc.page_content for doc in docsearch.similarity_search(row['question'], k=k)]\n",
    "                    scores = self.cross_encoder.predict([[question, doc] for doc in docs])\n",
    "                    scores = np.array(scores)[scores > 0.5]\n",
    "                    top_k_indices = scores.argsort()[::-1][:k]\n",
    "                    docs = [docs[i] for i in top_k_indices]\n",
    "                    context = model.context(docs)\n",
    "                    prompt = model.prompt(question, context)\n",
    "                    answer = model.query(prompt)\n",
    "                    answers.append({\n",
    "                        'index': index,\n",
    "                        'question': question,\n",
    "                        'right answer': row['answer'] + '. ' + row['option ' + row['answer'].upper()],\n",
    "                        'answer': answer,\n",
    "                    })\n",
    "                    end = time.time()\n",
    "                with open(f'answers_{chunk_size}_{k}_{j}.json', 'w') as f:\n",
    "                    json.dump({\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'k': k,\n",
    "                        'duration': end - start,\n",
    "                        'answers': answers,\n",
    "                    }, f)\n",
    "        print('Finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_kwargs {'torch_dtype': torch.float16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:31<00:00, 30.48s/it]\n"
     ]
    }
   ],
   "source": [
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = tester(model, path=\"documents\", chunk_size=[1000, 1000, 600, 600, 600], k=[2, 3, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Exam questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.test(df, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
